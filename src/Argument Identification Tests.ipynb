{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df476e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.tagging\n",
    "\n",
    "predictor_bert = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/structured-prediction-srl-bert.2020.12.15.tar.gz\")\n",
    "\n",
    "predictor_bilstm = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/openie-model.2020.03.26.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffc0ceed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verbs': [{'verb': 'went',\n",
       "   'description': '[ARG0: They] [V: went] [ARG1: on a hike in the woods] .',\n",
       "   'tags': ['B-ARG0',\n",
       "    'B-V',\n",
       "    'B-ARG1',\n",
       "    'I-ARG1',\n",
       "    'I-ARG1',\n",
       "    'I-ARG1',\n",
       "    'I-ARG1',\n",
       "    'I-ARG1',\n",
       "    'O']}],\n",
       " 'words': ['They', 'went', 'on', 'a', 'hike', 'in', 'the', 'woods', '.']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor_bert.predict(sentence=\"They went on a hike in the woods.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4e351c",
   "metadata": {},
   "source": [
    "### INV: Em dash statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cd07df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'original': 'I went to the store yesterday.', 'em_dash': 'I went to the store—where I found a great sale—yesterday.'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('data/argument_identification_em_dash_inv.json', 'r') as infile:\n",
    "    sentences = json.loads(infile.read())\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1a87b660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(BERT) INV - Argument detection invariance to em dash phrases failure rate:  0.9433962264150944\n",
      "(BILSTM) INV - Argument detection invariance to em dash phrases failure rate : 0.9622641509433962\n"
     ]
    }
   ],
   "source": [
    "# Define a function to extract the em dash (works with sentence that contain only one of such phrase)\n",
    "def extract_em_dash(sent):\n",
    "    # Check if the em dash exists in the sentence\n",
    "    if '—' in sent:\n",
    "        # Split the sentence into chunks using the em dash as a separator\n",
    "        chunks = sent.split('—')\n",
    "        # Return the second chunk (i.e., the text after the em dash)\n",
    "        return chunks[1]\n",
    "    \n",
    "# Define a function to compute the failure rate of the predictor for sentences with em dashes\n",
    "def em_dash_inv(predictor, sentences):\n",
    "    # Initialize a list to store failure rates\n",
    "    failure_rates = []\n",
    "    # Iterate through the sentences\n",
    "    for sentence in sentences:\n",
    "        # Extract the original and em dash versions of the sentence\n",
    "        original = sentence['original']\n",
    "        dashed = sentence['em_dash']\n",
    "        # Extract the contents of the em dash\n",
    "        em_dash = extract_em_dash(dashed)\n",
    "        \n",
    "        # Make predictions for the original and em dash sentences\n",
    "        original_pred = predictor.predict(original)\n",
    "        dashed_pred = predictor.predict(dashed)\n",
    "        \n",
    "        # Initialize sets to store argument detections for each sentence version\n",
    "        original_argument_detections = set()\n",
    "        dashed_argument_detections = set()\n",
    "        \n",
    "        \n",
    "        # Extract arguments from the original sentence predictions\n",
    "        for predicate in original_pred['verbs']:\n",
    "            verb = predicate['verb']\n",
    "            tags = predicate['tags']\n",
    "            \n",
    "            # Iterate through the tokens and tags in the prediction\n",
    "            for token, tag in zip(original_pred['words'], tags):\n",
    "                # Add arguments to the set of detections for the original sentence\n",
    "                if 'ARG' in tag:\n",
    "                    original_argument_detections.add((verb, token))\n",
    "                            \n",
    "        # Extract arguments from the em dash sentence predictions\n",
    "        for predicate in dashed_pred['verbs']:\n",
    "            verb = predicate['verb']\n",
    "            # Check if the verb is not in the em dash text\n",
    "            if verb not in em_dash:\n",
    "                tags = predicate['tags']\n",
    "                include = True\n",
    "                # Iterate through the tokens and tags in the prediction\n",
    "                for token, tag in zip(dashed_pred['words'], tags):\n",
    "                    # Toggle the inclusion flag when encountering an em dash\n",
    "                    if '—' in token:\n",
    "                        include = not include\n",
    "\n",
    "                    # Add arguments to the set of detections for the dashed sentence\n",
    "                    if 'ARG' in tag and include:\n",
    "                        dashed_argument_detections.add((verb, token))\n",
    "                    \n",
    "        # Compare the number of argument detections in each version and append the result to the failure rates list\n",
    "        failure_rates.append(len(dashed_argument_detections) != len(original_argument_detections))\n",
    "                    \n",
    "    # Calculate the overall failure rate by dividing the sum of failures by the total number of sentences\n",
    "    return (sum(failure_rates) / len(failure_rates))\n",
    "\n",
    "print(\"(BERT) INV - Argument detection invariance to em dash phrases failure rate: \", em_dash_inv(predictor_bert, sentences))\n",
    "print(\"(BILSTM) INV - Argument detection invariance to em dash phrases failure rate :\", em_dash_inv(predictor_bilstm, sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c53f72a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verbs': [{'verb': 'am',\n",
       "   'description': '[ARG1: I] [V: am] [ARG2: happy]',\n",
       "   'tags': ['B-ARG1', 'B-V', 'B-ARG2']}],\n",
       " 'words': ['I', 'am', 'happy']}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor_bilstm.predict_tokenized([\"I\", \"am\", \"happy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8fcc1aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_em_dash(\"Hello-world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c18cf016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8212"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('—')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allenNLP",
   "language": "python",
   "name": "allennlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
